# Hands and Objects

## Project overview
This project holds codes for training models to accomplish certain tasks. Using egocentric video inputs, the motivation of these tasks is to understand the camera wearer's activity in terms of hand and object interaction. [^ego4d]

[^ego4d]: K. Grauman, et al. [Ego4d: Around the world in 3,000 hours of egocentric video](https://arxiv.org/pdf/2110.07058.pdf).Â arXiv preprint arXiv:2110.07058, 2021.

## Tasks
1. [Objectness classification](https://github.com/pasca-l/hands-and-objects/tree/main/objectness_classification)
> Inspired by the "state change object detection" task, this task handles object detection in the level of semantic segmentation. The objective is to determine the object that is undergoing a state change at pixel level.

2. [Keypoint estimation](https://github.com/pasca-l/hands-and-objects/tree/main/keypoint_estimation)
> This task incorporates the "object state change classification" and the "PNR temporal localization" task, which handles keyframe detection given a video clip. The objective is to detect the keyframe where the object experiences the point of no return (PNR), in other words, the point at which the state of the object cannot be reversed.

## Usage
To use CUDA version of `pytorch` libraries, use the following command. This replaces the poetry registered CPU-only version to CUDA-version (version 11.8).
```shell
$ make gpu_setting
```
